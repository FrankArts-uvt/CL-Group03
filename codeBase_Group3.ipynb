{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from collections import Counter, defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make sure that the training_corpus.json file is in the same folder as this notebook.\n",
    "# When running for the first time, generated_training_corpus.json and generated_test_corpus.json do not exist yet.\n",
    "# They will be generated by running the code in the block below this one.\n",
    "original_path = 'training_corpus.json' #Original json file\n",
    "train_path = 'generated_training_corpus.json' #Used for storing generated training set (80%)\n",
    "test_path = 'generated_test_corpus.json' # Used for storing generated test set (20%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done splitting the sentences.\n",
      "Train set length:  495629\n",
      "Test set length:  496\n",
      "Writing the files starts now...\n",
      "Done writing the training set!\n",
      "Done writing the test set!\n"
     ]
    }
   ],
   "source": [
    "from random import randint\n",
    "#Need this to create random samples, hope this is ok.\n",
    "\n",
    "def split_training_test(train_percent):\n",
    "    #Splits the training set into a training set and test set.\n",
    "    #The input variable train_percent determines how large the training set is compared to the test set.\n",
    "    #For example: train_percent = 80.0 -> 80% of the sentences go to the training set, 20% to the test set.\n",
    "    train_sentences = json.load(open(original_path, 'r'))\n",
    "    test_sentences = []\n",
    "    test_target = int(len(train_sentences) * (1 - train_percent/100))\n",
    "    for i in range(test_target):\n",
    "        n = randint(0, len(train_sentences)-1)\n",
    "        test_sentences.append(train_sentences.pop(n))\n",
    "    print(\"Done splitting the sentences.\")\n",
    "    print(\"Train set length: \" , len(train_sentences))\n",
    "    print(\"Test set length: \" , len(test_sentences))\n",
    "    print(\"Writing the files starts now...\")\n",
    "    \n",
    "    with open(train_path, 'w') as train_file:\n",
    "        json.dump(train_sentences, train_file)\n",
    "    print(\"Done writing the training set!\")\n",
    "    with open(test_path, 'w') as test_file:\n",
    "        json.dump(test_sentences, test_file)\n",
    "    print(\"Done writing the test set!\")\n",
    "\n",
    "#Run the following command to generate a new training and test set:\n",
    "split_training_test(99.9) \n",
    "# Ideal ratio for training set would probably be around 80/20, \n",
    "# but larger test set means longer calculation for perplexity.\n",
    "# To speed up perplexity calculation, I set the train_percent to 99 \n",
    "#                                               (so test set is only 1% of the data, = 5.000 sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Corpus(object):\n",
    "    \n",
    "    \"\"\"\n",
    "    This class creates a corpus object read off a .json file consisting of a list of lists,\n",
    "    where each inner list is a sentence encoded as a list of strings.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, path, t, n, bos_eos=True, vocab=None):\n",
    "        \n",
    "        \"\"\"\n",
    "        DON'T TOUCH THIS CLASS! \n",
    "        IT'S HERE TO SHOW THE PROCESS, YOU DON'T NEED TO ANYTHING HERE. \n",
    "        \n",
    "        A Corpus object has the following attributes:\n",
    "         - vocab: set or None (default). If a set is passed, words in the input file not \n",
    "                         found in the set are replaced with the UNK string\n",
    "         - path: str, the path to the .json file used to build the corpus object\n",
    "         - t: int, words with frequency count < t are replaced with the UNK string\n",
    "         - ngram_size: int, 2 for bigrams, 3 for trigrams, and so on.\n",
    "         - bos_eos: bool, default to True. If False, bos and eos symbols are not \n",
    "                     prepended and appended to sentences.\n",
    "         - sentences: list of lists, containing the input sentences after lowercasing and \n",
    "                         splitting at the white space\n",
    "         - frequencies: Counter, mapping tokens to their frequency count in the corpus\n",
    "        \"\"\"\n",
    "        \n",
    "        self.vocab = vocab        \n",
    "        self.path = path\n",
    "        self.t = t\n",
    "        self.ngram_size = n\n",
    "        self.bos_eos = bos_eos\n",
    "        \n",
    "        self.sentences = self.read()\n",
    "        # output --> [['i', 'am', 'home' '.'], ['you', 'went', 'to', 'the', 'park', '.'], ...]\n",
    "    \n",
    "        self.frequencies = self.freq_distr()\n",
    "        # output --> Counter('the': 485099, 'of': 301877, 'i': 286549, ...)\n",
    "        # the numbers are made up, they aren't the actual frequency counts\n",
    "        \n",
    "        if self.t or self.vocab:\n",
    "            # input --> [['i', 'am', 'home' '.'], ['you', 'went', 'to', 'the', 'park', '.'], ...]\n",
    "            self.sentences = self.filter_words()\n",
    "            # output --> [['i', 'am', 'home' '.'], ['you', 'went', 'to', 'the', 'UNK', '.'], ...]\n",
    "            # supposing that park wasn't frequent enough or was outside of the training \n",
    "            # vocabulary, it gets replaced by the UNK string\n",
    "            \n",
    "        if self.bos_eos:\n",
    "            # input --> [['i', 'am', 'home' '.'], ['you', 'went', 'to', 'the', 'park', '.'], ...]\n",
    "            self.sentences = self.add_bos_eos()\n",
    "            # output --> [['bos', i', 'am', 'home' '.', 'eos'], \n",
    "            #             ['bos', you', 'went', 'to', 'the', 'park', '.', 'eos'], ...]\n",
    "                    \n",
    "    def read(self):\n",
    "        \n",
    "        \"\"\"\n",
    "        Reads the sentences off the .json file, replaces quotes, lowercases strings and splits \n",
    "        at the white space. Returns a list of lists.\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.path.endswith('.json'):\n",
    "            sentences = json.load(open(self.path, 'r'))                \n",
    "        else:   \n",
    "            sentences = []\n",
    "            with open(self.path, 'r', encoding='latin-1') as f:\n",
    "                for line in f:\n",
    "                    print(line[:20])\n",
    "                    # first strip away newline symbols and the like, then replace ' and \" with the empty \n",
    "                    # string and get rid of possible remaining trailing spaces \n",
    "                    line = line.strip().translate({ord(i): None for i in '\"\\'\\\\'}).strip(' ')\n",
    "                    # lowercase and split at the white space (the corpus has ben previously tokenized)\n",
    "                    sentences.append(line.lower().split(' '))\n",
    "        \n",
    "        return sentences\n",
    "    \n",
    "    def freq_distr(self):\n",
    "        \n",
    "        \"\"\"\n",
    "        Creates a counter mapping tokens to frequency counts\n",
    "        \n",
    "        count = Counter()\n",
    "        for sentence in self.sentences:\n",
    "            for word in sentence:\n",
    "                count[w] += 1\n",
    "            \n",
    "        \"\"\"\n",
    "    \n",
    "        return Counter([word for sentence in self.sentences for word in sentence])\n",
    "        \n",
    "    \n",
    "    def filter_words(self):\n",
    "        \n",
    "        \"\"\"\n",
    "        Replaces illegal tokens with the UNK string. A token is illegal if its frequency count\n",
    "        is lower than the given threshold and/or if it falls outside the specified vocabulary.\n",
    "        The two filters can be both active at the same time but don't have to be. To exclude the \n",
    "        frequency filter, set t=0 in the class call.\n",
    "        \"\"\"\n",
    "        \n",
    "        filtered_sentences = []\n",
    "        for sentence in self.sentences:\n",
    "            filtered_sentence = []\n",
    "            for word in sentence:\n",
    "                if self.t and self.vocab:\n",
    "                    # check that the word is frequent enough and occurs in the vocabulary\n",
    "                    filtered_sentence.append(\n",
    "                        word if self.frequencies[word] > self.t and word in self.vocab else 'UNK'\n",
    "                    )\n",
    "                else:\n",
    "                    if self.t:\n",
    "                        # check that the word is frequent enough\n",
    "                        filtered_sentence.append(word if self.frequencies[word] > self.t else 'UNK')\n",
    "                    else:\n",
    "                        # check if the word occurs in the vocabulary\n",
    "                        filtered_sentence.append(word if word in self.vocab else 'UNK')\n",
    "                        \n",
    "            if len(filtered_sentence) > 1:\n",
    "                # make sure that the sentence contains more than 1 token\n",
    "                filtered_sentences.append(filtered_sentence)\n",
    "    \n",
    "        return filtered_sentences\n",
    "    \n",
    "    def add_bos_eos(self):\n",
    "        \n",
    "        \"\"\"\n",
    "        Adds the necessary number of BOS symbols and one EOS symbol.\n",
    "        \n",
    "        In a bigram model, you need one bos and one eos; in a trigram model you need two bos and one eos, \n",
    "        and so on...\n",
    "        \"\"\"\n",
    "        \n",
    "        padded_sentences = []\n",
    "        for sentence in self.sentences:\n",
    "            padded_sentence = ['#bos#']*(self.ngram_size-1) + sentence + ['#eos#']\n",
    "            padded_sentences.append(padded_sentence)\n",
    "    \n",
    "        return padded_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LM(object):\n",
    "    \n",
    "    \"\"\"\n",
    "    Creates a language model object which can be trained and tested.\n",
    "    The language model has the following attributes:\n",
    "     - vocab: set of strings\n",
    "     - lam: float, indicating the constant to add to transition counts to smooth them (default to 1)\n",
    "     - ngram_size: int, the size of the ngrams\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n, vocab=None, smooth='Laplace', lam=1, lambdas = [-1]):\n",
    "        \n",
    "        self.vocab = vocab\n",
    "        self.lam = lam\n",
    "        self.ngram_size = n\n",
    "        self.smooth = smooth\n",
    "        self.counts = [-1]\n",
    "        if not smooth == 'Laplace':\n",
    "            #If we are not using LaPlace smoothing, that means we want to use discounting.\n",
    "            #To do this, we need lambda values to determine the weight of different ngram sizes.\n",
    "            self.lambdas = lambdas\n",
    "            #lambdas contains a list of weights, all add up to 1.\n",
    "            #The exact values of the weights need to be finetuned.\n",
    "            #lambda[0] is used as the weight for ngram_size == 1 (target frequency)\n",
    "            #lambda[1] is used as the weight for ngram_size == 2 (history of size 1)\n",
    "            #etc..\n",
    "        \n",
    "    def get_ngram(self, sentence, i, size=-1):\n",
    "        \n",
    "        \"\"\"\n",
    "        CHANGE AT OWN RISK.\n",
    "        \n",
    "        Takes in a list of string and an index, and returns the history and current \n",
    "        token of the appropriate size: the current token is the one at the provided \n",
    "        index, while the history consists of the n-1 previous tokens. If the ngram \n",
    "        size is 1, only the current token is returned.\n",
    "        \n",
    "        Example:\n",
    "        input sentence: ['bos', 'i', 'am', 'home', 'eos']\n",
    "        target index: 2\n",
    "        ngram size: 3\n",
    "        \n",
    "        ngram = ['bos', 'i', 'am']  \n",
    "        #from index 2-(3-1) = 0 to index i (the +1 is just because of how Python slices lists) \n",
    "        \n",
    "        history = ('bos', 'i')\n",
    "        target = 'am'\n",
    "        return (('bos', 'i'), 'am')\n",
    "        \"\"\"\n",
    "        # We have added the size parameter to this function. \n",
    "        # This can be used to request ngrams of different sizes (useful for discounting)\n",
    "        # In case the size parameter is not explicitly defined, the following if statement sets it to self.ngram_size\n",
    "        #  so that the function still works like it did before our modification.\n",
    "        if size == -1:\n",
    "            size = self.ngram_size\n",
    "        \n",
    "        if size == 1:\n",
    "            return sentence[i]\n",
    "        else:\n",
    "            ngram = sentence[i-(size-1):i+1]\n",
    "            history = tuple(ngram[:-1])\n",
    "            target = ngram[-1]\n",
    "            return (history, target)\n",
    "    \n",
    "    def update_multiple_counts(self, corpusList):\n",
    "        # This function is used to call update_counts multiple times, \n",
    "        #  when multiple counts matrices are needed for different ngram sizes (for discounting)\n",
    "        n = len(corpusList)\n",
    "        self.counts = [-1]*n\n",
    "        for i in range(n):\n",
    "            self.update_counts(corpusList[i], i)\n",
    "            \n",
    "            \n",
    "    \n",
    "    def update_counts(self, corpus, corpusIndex = 0):\n",
    "        \n",
    "        \"\"\"\n",
    "        CHANGE AT OWN RISK.\n",
    "        \n",
    "        Creates a transition matrix with counts in the form of a default dict mapping history\n",
    "        states to current states to the co-occurrence count (unless the ngram size is 1, in which\n",
    "        case the transition matrix is a simple counter mapping tokens to frequencies. \n",
    "        The ngram size of the corpus object has to be the same as the language model ngram size.\n",
    "        The input corpus (passed by providing the corpus object) is processed by extracting ngrams\n",
    "        of the chosen size and updating transition counts.\n",
    "        \n",
    "        This method creates three attributes for the language model object:\n",
    "         - counts: dict, described above\n",
    "         - vocab: set, containing all the tokens in the corpus\n",
    "         - vocab_size: int, indicating the number of tokens in the vocabulary\n",
    "        \"\"\"\n",
    "        #Commented out the following section for now, as this check does not work with the new LM that uses mutliple counts matrices.\n",
    "        \"\"\"if self.ngram_size != corpus.ngram_size:\n",
    "            raise ValueError(\"The corpus was pre-processed considering an ngram size of {} while the \"\n",
    "                             \"language model was created with an ngram size of {}. \\n\"\n",
    "                             \"Please choose the same ngram size for pre-processing the corpus and fitting \"\n",
    "                             \"the model.\".format(corpus.ngram_size, self.ngram_size))\"\"\"\n",
    "        \n",
    "        self.counts[corpusIndex] = defaultdict(dict) if corpus.ngram_size > 1 else Counter()\n",
    "        for sentence in corpus.sentences:\n",
    "            for idx in range(corpus.ngram_size-1, len(sentence)):\n",
    "                ngram = self.get_ngram(sentence, idx, corpus.ngram_size)\n",
    "                if corpus.ngram_size == 1:\n",
    "                    self.counts[corpusIndex][ngram] += 1\n",
    "                else:\n",
    "                    # it's faster to try to do something and catch an exception than to use an if statement to check\n",
    "                    # whether a condition is met beforehand. The if is checked everytime, the exception is only catched\n",
    "                    # the first time, after that everything runs smoothly\n",
    "                    try:\n",
    "                        self.counts[corpusIndex][ngram[0]][ngram[1]] += 1\n",
    "                    except KeyError:\n",
    "                        self.counts[corpusIndex][ngram[0]][ngram[1]] = 1\n",
    "        \n",
    "        # first loop through the sentences in the corpus, than loop through each word in a sentence\n",
    "        self.vocab = {word for sentence in corpus.sentences for word in sentence}\n",
    "        self.vocab_size = len(self.vocab)\n",
    "    \n",
    "    def get_unigram_probability(self, ngram):\n",
    "        \n",
    "        \"\"\"\n",
    "        CHANGE THIS.\n",
    "        \n",
    "        Compute the probability of a given unigram in the estimated language model using\n",
    "        Laplace smoothing (add k). \n",
    "        \"\"\"\n",
    "        # We have added an if statement that handles this function differently when the smoothing method is not \n",
    "        #  set to 'Laplace'. The alternative does not use add-K smoothing, but if somehow the ngram count still\n",
    "        #  results in 0, we add 0.000001\n",
    "        if self.smooth == 'Laplace':\n",
    "            tot = sum(list(self.counts[0].values())) + (self.vocab_size*self.lam)\n",
    "            try:\n",
    "            #This generally triggers if the history-list exists in the counts matrix\n",
    "                ngram_count = self.counts[0][ngram] + self.lam\n",
    "            except KeyError:\n",
    "            #This exception triggers if the history-list does not exist in the counts matrix. \n",
    "            #This should never happen as all UNIgrams either appear in the counts matrix or are replaced by UNK.\n",
    "                ngram_count = self.lam\n",
    "                print(ngram_count, tot)\n",
    "\n",
    "            return ngram_count/tot\n",
    "        else: #If self.smooth is not Laplace, apply discounting. \n",
    "            #   For unigram, that means just using the frequency of the unigram, without adding k.\n",
    "            tot = sum(list(self.counts[0].values()))\n",
    "            try:\n",
    "                ngram_count = self.counts[0][ngram]\n",
    "                if ngram_count == 0:\n",
    "                    ngram_count = 0.000001\n",
    "            except KeyError:\n",
    "                #This should never happen, as we were promised that all words are in the vocabulary \n",
    "                # (and thus in the unigram counts matrix)\n",
    "                ngram_count = 0.000001\n",
    "                print(\"Error, word not in vocabulary\")\n",
    "            return ngram_count/tot\n",
    "\n",
    "    \n",
    "    def get_ngram_probability(self, history, target):\n",
    "        \"\"\"\n",
    "        IF self.smooth == 'Laplace': \n",
    "            Compute the conditional probability of the target token given the history, \n",
    "                using Laplace smoothing (add k).\n",
    "        ELSE:\n",
    "            Compute the conditional probability of the target token given the history, \n",
    "                using Discounting. This means computing the conditional probability of the target token given the history,\n",
    "                and multiplying this value by its lambda weight. This is repeated recursively with smalller and smaller\n",
    "                history lists (history[1:]), until len(history) is 0, indicating that a unigram is reached. Finally,\n",
    "                get_unigram_probability is called once to find the unigram probability, and all scores are combined.\n",
    "        \"\"\"\n",
    "        if self.smooth == 'Laplace':\n",
    "            try:\n",
    "                #This generally triggers if the history-list exists in the counts matrix\n",
    "                ngram_tot = np.sum(list(self.counts[0][history].values())) + (self.vocab_size*self.lam)\n",
    "                #ngram_tot is used to estimate the probability in the last step of this function.\n",
    "                #it stands for the sum of all frequencies of each engram, + the total added smoothing.\n",
    "                try:\n",
    "                    #This try part goes through if both the history and the target exist in the counts matrix.\n",
    "                    transition_count = self.counts[0][history][target] + self.lam\n",
    "                except KeyError:\n",
    "                    #If the history exists but the target does not exist, set transition count to k:\n",
    "                    transition_count = self.lam\n",
    "            except KeyError:\n",
    "                #This exception triggers if the history-list does not exist in the counts matrix\n",
    "                transition_count = self.lam\n",
    "                ngram_tot = self.vocab_size*self.lam\n",
    "\n",
    "            return transition_count/ngram_tot\n",
    "        else:\n",
    "            #If smoothing is not set to Laplace, apply Discounting\n",
    "            history_size = len(history)\n",
    "            if history_size == 0: #If there was no history, this is a unigram\n",
    "                return self.lambdas[0] * self.get_unigram_probability(target) \n",
    "                #Then return the unigram probability, multiplied by the lambda weight\n",
    "            else:\n",
    "                try:\n",
    "                    #If the history engram does exist in counts:\n",
    "                    ngram_tot = np.sum(list(self.counts[history_size][history].values()))\n",
    "                    try: \n",
    "                        #And if the target also exists in counts:\n",
    "                        transition_count = self.counts[history_size][history][target]\n",
    "                    except KeyError:\n",
    "                        #If the history engram does exist in counts, but target does not:\n",
    "                        transition_count = 0\n",
    "                except KeyError:\n",
    "                    #If the history engram does not exist in counts:\n",
    "                    transition_count = 0\n",
    "                    ngram_tot = 1\n",
    "            if ngram_tot == 0:\n",
    "                transition_count = 0\n",
    "                ngram_tot = 1\n",
    "            #Calculate the probability of target appearing after history based only on this ngram size:\n",
    "            ngram_prob = transition_count/ngram_tot \n",
    "            #Multiply it by the corresponding weight, and add the weighted scores of the lower ngram sizes:\n",
    "            return self.lambdas[history_size] * ngram_prob + self.get_ngram_probability(history[1:], target)\n",
    "                \n",
    "            \n",
    "    def setLambdas(self, lambdas):\n",
    "        # Used to change the lambdas without having to re-count the counts matrices, or rebuild corpi, etc.\n",
    "        self.lambdas = lambdas\n",
    "    \n",
    "    def perplexity(self, test_corpus):\n",
    "        \"\"\"\n",
    "        Uses the estimated language model to process a corpus and computes the perplexity \n",
    "        of the language model over the corpus.\n",
    "        \n",
    "        DON'T TOUCH THIS FUNCTION!!!\n",
    "        \"\"\"\n",
    "        \n",
    "        probs = []\n",
    "        for sentence in test_corpus.sentences:\n",
    "            for idx in range(self.ngram_size-1, len(sentence)):\n",
    "                ngram = self.get_ngram(sentence, idx)\n",
    "                if self.ngram_size == 1:\n",
    "                    probs.append(self.get_unigram_probability(ngram))\n",
    "                else:\n",
    "                    probs.append(self.get_ngram_probability(ngram[0], ngram[1]))\n",
    "        entropy = np.log2(probs)\n",
    "        # this assertion makes sure that you retrieved valid probabilities, whose log must be <= 0\n",
    "        assert all(entropy <= 0)\n",
    "        \n",
    "        avg_entropy = -1 * (sum(entropy) / len(entropy))\n",
    "        \n",
    "        return pow(2.0, avg_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus created..\n",
      "Language Model created..\n",
      "Counts updated..\n",
      "Test_corpus created..\n",
      "Perplexity calculated! Perplexity of the unigram model: 816.230178051716\n"
     ]
    }
   ],
   "source": [
    "# example code to run a unigram model with add 0.001 smoothing. Tokens with a frequency count lower than 10\n",
    "# are replaced with the UNK string\n",
    "n = 1\n",
    "train_corpus = Corpus(train_path, 10, n, bos_eos=True, vocab=None)\n",
    "print(\"Corpus created..\")\n",
    "unigram_model = LM(n, lam=0.001)\n",
    "print(\"Language Model created..\")\n",
    "unigram_model.update_counts(train_corpus)\n",
    "print(\"Counts updated..\")\n",
    "\n",
    "# to ensure consistency, the test corpus is filtered using the vocabulary of the trained language model\n",
    "test_corpus = Corpus(test_path, None, n, bos_eos=True, vocab=unigram_model.vocab)\n",
    "print(\"Test_corpus created..\")\n",
    "p = unigram_model.perplexity(test_corpus)\n",
    "print(\"Perplexity calculated! Perplexity of the unigram model: {}\".format(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus created..\n",
      "Language Model created..\n",
      "Counts updated..\n",
      "Test_corpus created..\n",
      "Perplexity calculated! Perplexity of the unigram model: 816.2301205179506\n"
     ]
    }
   ],
   "source": [
    "# Calculating perplexity for the same model, but using discounting:\n",
    "#Note, this should be roughly the same, as no discounting can occur with an engram size of 1\n",
    "n = 1\n",
    "train_corpus = Corpus(train_path, 10, n, bos_eos=True, vocab=None)\n",
    "print(\"Corpus created..\")\n",
    "unigram_model = LM(n, lam=0.001, smooth = 'discount', lambdas = [1])\n",
    "print(\"Language Model created..\")\n",
    "unigram_model.update_counts(train_corpus)\n",
    "print(\"Counts updated..\")\n",
    "\n",
    "# to ensure consistency, the test corpus is filtered using the vocabulary of the trained language model\n",
    "test_corpus = Corpus(test_path, None, n, bos_eos=True, vocab=unigram_model.vocab)\n",
    "print(\"Test_corpus created..\")\n",
    "p = unigram_model.perplexity(test_corpus)\n",
    "print(\"Perplexity calculated! Perplexity of the unigram model: {}\".format(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus created..\n",
      "Language Model created..\n",
      "Counts updated..\n",
      "Test_corpus created..\n",
      "Perplexity calculated! Perplexity of the bigram model: 293.21398546789186\n"
     ]
    }
   ],
   "source": [
    "# example code to run a bigram model with add 0.001 smoothing. The same frequency threshold is applied.\n",
    "n = 2\n",
    "train_corpus = Corpus(train_path, 10, n, bos_eos=True, vocab=None)\n",
    "print(\"Corpus created..\")\n",
    "bigram_model = LM(n, lam=0.001)\n",
    "print(\"Language Model created..\")\n",
    "bigram_model.update_counts(train_corpus)\n",
    "print(\"Counts updated..\")\n",
    "\n",
    "# to ensure consistency, the test corpus is filtered using the vocabulary of the trained language model\n",
    "test_corpus = Corpus(test_path, None, n, bos_eos=True, vocab=bigram_model.vocab)\n",
    "print(\"Test_corpus created..\")\n",
    "p = bigram_model.perplexity(test_corpus)\n",
    "print(\"Perplexity calculated! Perplexity of the bigram model: {}\".format(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus list created..\n",
      "Language Model created..\n",
      "Counts updated..\n",
      "Test_corpus created..\n",
      "Perplexity calculated! Perplexity of the bigram model: 239.6711354689498\n"
     ]
    }
   ],
   "source": [
    "n = 2\n",
    "train_corpus_list = []\n",
    "for i in range(n):\n",
    "    train_corpus_list.append(Corpus(train_path, 10, i+1, bos_eos=True, vocab = None))\n",
    "print(\"Corpus list created..\")\n",
    "bigram_model = LM(n, lam=0.001, smooth = 'discount', lambdas = [0.2, 0.8])\n",
    "print(\"Language Model created..\")\n",
    "\n",
    "bigram_model.update_multiple_counts(train_corpus_list)\n",
    "print(\"Counts updated..\")\n",
    "\n",
    "# to ensure consistency, the test corpus is filtered using the vocabulary of the trained language model\n",
    "test_corpus = Corpus(test_path, None, n, bos_eos=True, vocab=bigram_model.vocab)\n",
    "print(\"Test_corpus created..\")\n",
    "p = bigram_model.perplexity(test_corpus)\n",
    "print(\"Perplexity calculated! Perplexity of the bigram model: {}\".format(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus created..\n",
      "Language Model created..\n",
      "Counts updated..\n",
      "Test_corpus created..\n",
      "Perplexity calculated! Perplexity of the trigram model: 988.3821617822591\n"
     ]
    }
   ],
   "source": [
    "# example code to run a trigram model with add 0.001 smoothing. The same frequency threshold is applied.\n",
    "n = 3\n",
    "train_corpus = Corpus(train_path, 10, n, bos_eos=True, vocab=None)\n",
    "print(\"Corpus created..\")\n",
    "trigram_model = LM(n, lam=0.001)\n",
    "print(\"Language Model created..\")\n",
    "trigram_model.update_counts(train_corpus)\n",
    "print(\"Counts updated..\")\n",
    "\n",
    "# to ensure consistency, the test corpus is filtered using the vocabulary of the trained language model\n",
    "test_corpus = Corpus(test_path, None, n, bos_eos=True, vocab=trigram_model.vocab)\n",
    "print(\"Test_corpus created..\")\n",
    "p = trigram_model.perplexity(test_corpus)\n",
    "print(\"Perplexity calculated! Perplexity of the trigram model: {}\".format(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus list created..\n",
      "Language Model created..\n",
      "Counts updated..\n",
      "Test_corpus created..\n",
      "Perplexity calculated! Perplexity of the trigram model: 200.7484293639936\n"
     ]
    }
   ],
   "source": [
    "n = 3\n",
    "train_corpus_list = []\n",
    "for i in range(n):\n",
    "    train_corpus_list.append(Corpus(train_path, 10, i+1, bos_eos=True, vocab = None))\n",
    "print(\"Corpus list created..\")\n",
    "trigram_model = LM(n, lam=0.001, smooth = 'discount', lambdas = [0.225, 0.55, 0.225]) #0.1, 0.3, 0.6 -> 222 perplexity\n",
    "print(\"Language Model created..\")\n",
    "\n",
    "trigram_model.update_multiple_counts(train_corpus_list)\n",
    "print(\"Counts updated..\")\n",
    "\n",
    "# to ensure consistency, the test corpus is filtered using the vocabulary of the trained language model\n",
    "test_corpus = Corpus(test_path, None, n, bos_eos=True, vocab=trigram_model.vocab)\n",
    "print(\"Test_corpus created..\")\n",
    "p = trigram_model.perplexity(test_corpus)\n",
    "print(\"Perplexity calculated! Perplexity of the trigram model: {}\".format(p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best performing model:\n",
    "###### Building the corpi:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus 1 out of 5 has been created..\n",
      "Corpus 2 out of 5 has been created..\n",
      "Corpus 3 out of 5 has been created..\n",
      "Corpus 4 out of 5 has been created..\n",
      "Corpus 5 out of 5 has been created..\n",
      "Corpus list created..\n"
     ]
    }
   ],
   "source": [
    "n = 5\n",
    "train_corpus_list = []\n",
    "for i in range(n):\n",
    "    train_corpus_list.append(Corpus(train_path, 10, i+1, bos_eos=True, vocab = None))\n",
    "    print(\"Corpus {} out of {} has been created..\".format(i+1, n))\n",
    "print(\"Corpus list created..\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Creating the language model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language Model created..\n",
      "Counts updated..\n",
      "Test_corpus created..\n",
      "Perplexity calculated! Perplexity of the ngram model: 200.82333069237947\n",
      "Lambda's used: [0.15, 0.5, 0.2, 0.1, 0.05]\n"
     ]
    }
   ],
   "source": [
    "lambdas = [0.15, 0.50, 0.2, 0.1, 0.05]\n",
    "ngram_model = LM(n, lam=0.001, smooth = 'discount', lambdas = lambdas)\n",
    "print(\"Language Model created..\")\n",
    "\n",
    "ngram_model.update_multiple_counts(train_corpus_list)\n",
    "print(\"Counts updated..\")\n",
    "\n",
    "# to ensure consistency, the test corpus is filtered using the vocabulary of the trained language model\n",
    "test_corpus = Corpus(test_path, None, n, bos_eos=True, vocab=ngram_model.vocab)\n",
    "print(\"Test_corpus created..\")\n",
    "p = ngram_model.perplexity(test_corpus)\n",
    "print(\"Perplexity calculated! Perplexity of the ngram model: {}\".format(p))\n",
    "print(\"Lambda's used: {}\".format(lambdas))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Finetuning the hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity calculated! Perplexity of the ngram model: 197.05030981240427\n",
      "Lambda's used: [0.18, 0.55, 0.18, 0.05, 0.04]\n"
     ]
    }
   ],
   "source": [
    "ngram_model.setLambdas([0.18, 0.55, 0.18, 0.05, 0.04])\n",
    "p = ngram_model.perplexity(test_corpus)\n",
    "print(\"Perplexity calculated! Perplexity of the ngram model: {}\".format(p))\n",
    "print(\"Lambda's used: {}\".format(ngram_model.lambdas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average frequency of each history engram in the counts 1 matrix is: 343.01509314808965\n",
      "The average frequency of each history engram in the counts 2 matrix is: 5.9779641672839094\n",
      "The average frequency of each history engram in the counts 3 matrix is: 1.9367961181665696\n",
      "The average frequency of each history engram in the counts 4 matrix is: 1.3445868169375572\n",
      "The average frequency of each history engram in the counts 5 matrix is: 1.2185491853222503\n",
      "The average frequency of each history engram in the counts 6 matrix is: 1.190698265789514\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "# Checking how unique different length history ngrams are on average:\n",
    "for n_size in range(1, len(ngram_model.counts)):\n",
    "    counts_n = ngram_model.counts[n_size]\n",
    "    keys = counts_n.keys()\n",
    "    length = len(keys)\n",
    "    total = 0\n",
    "    for key in keys:\n",
    "        history_dict = counts_n[key]\n",
    "        for hkey in history_dict.keys():\n",
    "            total += history_dict[hkey]\n",
    "    print(\"The average frequency of each history engram in the counts {} matrix is: {}\".format(n_size, total/length))\n",
    "# Output:\n",
    "# n = 2:\n",
    "# The average frequency of each history engram in the counts 1 matrix is: 343.01509314808965\n",
    "# -> Indicating that on average, any history engram of size 1 (single word) appears 343 times in the corpus\n",
    "# n = 3:\n",
    "# The average frequency of each history engram in the counts 2 matrix is: 5.9779641672839094\n",
    "# -> Indicating that on average, any history engram of size 2 (two words) appears ~6 times in the corpus\n",
    "# n = 4: \n",
    "# The average frequency of each history engram in the counts 3 matrix is: 1.9367961181665696\n",
    "# -> Indicating that on average, any history engram of size 3 appears ~2 times in the corpus\n",
    "# n = 5:\n",
    "# The average frequency of each history engram in the counts 4 matrix is: 1.3445868169375572\n",
    "# -> Indicating that on average, any history engram of size 4 appears ~1.34 times in the corpus\n",
    "# --> So about 1 in 3 history engrams of size 4 appears more than once (not exactly, but helps to think this way)\n",
    "\n",
    "# n = 6:\n",
    "# The average frequency of each history engram in the counts 5 matrix is: 1.2185491853222503\n",
    "# -> Indicating that on average, any history engram of size 5 appears ~1.22 times in the corpus\n",
    "# --> So about 1 in 4.6 history engrams of size 4 appears more than once (not exactly, but helps to think this way)\n",
    "# n = 7:\n",
    "# The average frequency of each history engram in the counts 6 matrix is: 1.190698265789514\n",
    "# -> Indicating that on average, any history engram of size 6 appears ~1.2 times in the corpus\n",
    "# --> So about 1 in 5 history engrams of size 4 appears more than once (not exactly, but helps to think this way)\n",
    "    \n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
